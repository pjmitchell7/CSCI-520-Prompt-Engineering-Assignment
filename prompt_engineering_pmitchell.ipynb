{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8V4JxL62rQo",
        "outputId": "b8c412be-62e1-4925-fda6-4109c605f6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "# pip install openai\n",
        "!pip3 install openai\n",
        "\n",
        "#!pip3 install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/Homework3'\n",
        "prompt_dir = os.path.join(base_dir, 'prompts')\n",
        "os.makedirs(prompt_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "FQplO8KB7eKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0f42b4-4457-4459-caf1-ee3e4eecc3a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YSeCgVZx2rQq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "# Set it as an environment variable\n",
        "os.environ[\"GITHUB_TOKEN\"] = \"omitted\" # my personal key\n",
        "# To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n",
        "client = OpenAI(\n",
        "    base_url=\"https://models.inference.ai.azure.com\",\n",
        "    api_key=os.environ[\"GITHUB_TOKEN\"],\n",
        ")\n",
        "\n",
        "# List of available models\n",
        "model_choices = [\"Codestral-2501, gpt-4o-mini\"]\n",
        "#DeepSeek is not allowed on gov owened machines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    1: \"Zero-Shot\",\n",
        "    2: \"Few-Shot\",\n",
        "    3: \"Chain-of-Thought\",\n",
        "    4: \"Self-Consistency\",\n",
        "    5: \"Prompt Chaining\",\n",
        "    6: \"Role-Playing\""
      ],
      "metadata": {
        "id": "hQerCqW_7zWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Start checkpoint\n",
        "start_task = 21\n",
        "prompt_dir = \"/content/drive/MyDrive/Homework3/prompts\"\n",
        "\n",
        "# Format: (task, strategy, first or second, prompt type)\n",
        "prompts = [\n",
        "  # Task 21\n",
        "  (21, 1, 3, \"Read through this script and figure out what could go wrong logically. Then rewrite it to be more correct and easier to follow.\"),\n",
        "  (21, 2, 6, \"Pretend as if you're doing a code review on me. Identify any potential bugs or design flaws in this Python utility, then suggest a cleaner version that fixes those issues.\"),\n",
        "\n",
        "  # Task 22\n",
        "  (22, 1, 5, \"First figure out what this script is trying to do and then improve it so it cleans each line properly and counts words without error.\"),\n",
        "  (22, 2, 2, \"Example: Input-> 'Hello, world!' -> ['hello', 'world'] Now finish this file-processing function to clean text and count words reliably.\"),\n",
        "]\n",
        "\n",
        "# Process prompts\n",
        "for task, strategy, prompt_type, prompt_text in prompts:\n",
        "    base_filename = f\"task{task:02d}_strategy{strategy}\"\n",
        "    new_filename = f\"{base_filename}-{prompt_type}.txt\"\n",
        "    new_filepath = os.path.join(prompt_dir, new_filename)\n",
        "\n",
        "    # Write prompt\n",
        "    with open(new_filepath, \"w\") as f:\n",
        "        f.write(prompt_text.strip())\n",
        "    print(f\"Saved: {new_filename}\")\n",
        "\n",
        "    # Remove any old file\n",
        "    old_filepath = os.path.join(prompt_dir, f\"{base_filename}.txt\")\n",
        "    if os.path.exists(old_filepath):\n",
        "        os.remove(old_filepath)\n",
        "        print(f\"Removed old file: {base_filename}.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-RImgU5Esa_",
        "outputId": "f3bf90fb-3e62-4b10-81c4-f6db97452167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: task21_strategy1-3.txt\n",
            "Removed old file: task21_strategy1.txt\n",
            "Saved: task21_strategy2-6.txt\n",
            "Removed old file: task21_strategy2.txt\n",
            "Saved: task22_strategy1-5.txt\n",
            "Removed old file: task22_strategy1.txt\n",
            "Saved: task22_strategy2-2.txt\n",
            "Removed old file: task22_strategy2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths defined\n",
        "code_csv_path = \"/content/drive/MyDrive/Homework3/assignment3_task_code_formatted.csv\"\n",
        "prompts_path = \"/content/drive/MyDrive/Homework3/prompts\"\n",
        "\n",
        "# Read the CSV of code samples\n",
        "code_df = pd.read_csv(code_csv_path)\n",
        "\n",
        "# List all prompt files\n",
        "prompt_files = sorted(f for f in os.listdir(prompts_path) if f.endswith(\".txt\"))\n",
        "\n",
        "# Strategy label mapping\n",
        "strategy_map = {\n",
        "    1: \"Zero-Shot\",\n",
        "    2: \"Few-Shot\",\n",
        "    3: \"Chain-of-Thought\",\n",
        "    4: \"Self-Consistency\",\n",
        "    5: \"Prompt Chaining\",\n",
        "    6: \"Role-Playing\"\n",
        "}\n",
        "\n",
        "# model_choices[0] = \"Codestral, gpt-4o-mini\"\n",
        "codestral_model = model_choices[0].split(\",\")[0].strip()\n",
        "gpt4o_model = model_choices[0].split(\",\")[1].strip()\n",
        "\n",
        "records = []\n",
        "\n",
        "# Loop through all prompts\n",
        "for fname in prompt_files:\n",
        "    parts = fname.replace(\".txt\", \"\").split(\"_\")\n",
        "    task_num = int(parts[0].replace(\"task\", \"\"))\n",
        "    strat_num = int(parts[1].split(\"-\")[1])\n",
        "    strategy_label = strategy_map.get(strat_num, f\"Strategy-{strat_num}\")\n",
        "\n",
        "    # Read prompt text\n",
        "    with open(os.path.join(prompts_path, fname), \"r\") as f:\n",
        "        prompt_text = f.read().strip()\n",
        "\n",
        "    # Lookup corresponding code snippet\n",
        "    code_block = code_df.iloc[task_num - 1][\"Code\"]\n",
        "    full_input = f\"{prompt_text}\\n\\n```python\\n{code_block}\\n```\"\n",
        "\n",
        "    # Model 1: Codestral-2501\n",
        "    try:\n",
        "        codestral_response = client.chat.completions.create(\n",
        "            model=codestral_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": full_input}],\n",
        "            max_tokens=1024,\n",
        "            temperature=0.7,\n",
        "        ).choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        codestral_response = f\"[ERROR from {codestral_model}]: {e}\"\n",
        "\n",
        "    # Model 2: GPT-4o-mini\n",
        "    try:\n",
        "        gpt4o_response = client.chat.completions.create(\n",
        "            model=gpt4o_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": full_input}],\n",
        "            max_tokens=1024,\n",
        "            temperature=0.7,\n",
        "        ).choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        gpt4o_response = f\"[ERROR from {gpt4o_model}]: {e}\"\n",
        "\n",
        "    # Save both model outputs side-by-side\n",
        "    records.append({\n",
        "        \"Task\": f\"{task_num:02d}\",\n",
        "        \"Strategy\": strategy_label,\n",
        "        \"Temperature\": 0.7,\n",
        "        \"Prompt\": prompt_text,\n",
        "        \"Codestral Output\": codestral_response,\n",
        "        \"GPT-4o-mini Output\": gpt4o_response,\n",
        "    })\n",
        "\n",
        "# Save to CSV\n",
        "output_path = \"/content/drive/MyDrive/Homework3/Assignment3_Results.csv\"\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Results in: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiXiXKI8VDap",
        "outputId": "f02dd4db-7585-4bc7-a87b-52f6cebcbf2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results in: /content/drive/MyDrive/Homework3/Assignment3_Results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update Table to Calculate BLEU scores between models"
      ],
      "metadata": {
        "id": "by7woET0yL1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "input_csv = \"/content/drive/MyDrive/Homework3/Assignment3_Results.csv\"\n",
        "output_csv = \"/content/drive/MyDrive/Homework3/Assignment3_Results_WithBLEU.csv\"\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# function to compute bleu\n",
        "def compute_bleu(reference, candidate):\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    try:\n",
        "        score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n",
        "    except ZeroDivisionError:\n",
        "        score = 0.0\n",
        "    return score\n",
        "\n",
        "# Compute bleu for each row\n",
        "bleu_scores = []\n",
        "for idx, row in df.iterrows():\n",
        "    ref_output = str(row['GPT-4o-mini Output'])\n",
        "    cand_output = str(row['Codestral-2501 Output'])\n",
        "    bleu = compute_bleu(ref_output, cand_output)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "# Add new column\n",
        "df['BLEU Score'] = bleu_scores\n",
        "\n",
        "# save updated csv\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"Saved updated file with BLEU scores: {output_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVKJRvYCPUHl",
        "outputId": "93143a0d-6e22-426e-86cd-f06543124358"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved updated file with BLEU scores: /content/drive/MyDrive/Homework3/Assignment3_Results_WithBLEU.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}